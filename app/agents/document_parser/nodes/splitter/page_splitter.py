from __future__ import annotations

import re
from dataclasses import dataclass
from pathlib import Path
from typing import List

from bs4 import BeautifulSoup, Tag

from langchain_core.documents import Document
from langchain_upstage.document_parse import OutputFormat

from app.agents.document_parser.constants import TERMS_DIR


product_name_by_code = {
    ("meritz", "1"): "á„†á…¦á„…á…µá„á…³ á„†á…¡á„‹á…³á†·á„ƒá…³á†«á„ƒá…³á†« á„‡á…¡á†«á„…á…§á„ƒá…©á†¼á„†á…®á†¯á„‡á…©á„’á…¥á†·",
    ("meritz", "2"): "á„†á…®á„‡á…¢á„ƒá…¡á†¼ á„‘á…¦á†ºá„‘á…¥á„†á…µá†«á„á…³ Puppy&Familyá„‡á…©á„’á…¥á†· á„ƒá…¡á„‹á…µá„…á…¦á†¨á„á…³2601",
    ("meritz", "3"): "á„†á…®á„‡á…¢á„ƒá…¡á†¼ á„‘á…¦á†ºá„‘á…¥á„†á…µá†«á„á…³ Cat&Familyá„‡á…©á„’á…¥á†· á„ƒá…¡á„‹á…µá„…á…¦á†¨á„á…³2601",
}

output_extension_by_format = {
    "html": "html",
    "text": "txt",
    "markdown": "md",
}


@dataclass
class PageDoc:
    page: int
    text: str
    anchor_ids: List[str]


_PAGE_RE = re.compile(r"^\s*(?:-\s*)?(\d{1,4})(?:\s*-\s*)?\s*$")


def _norm_text(s: str) -> str:
    s = s.replace("\u00a0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()


def _extract_table_text(table_tag: Tag) -> str:
    """
    <table> â†’ í–‰(row) ë‹¨ìœ„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    ê° ì…€(cell)ì€ ' | ' ë¡œ ì—°ê²°
    """
    rows = []

    for tr in table_tag.find_all("tr", recursive=True):
        cells = []
        for cell in tr.find_all(["td", "th"], recursive=False):
            text = cell.get_text(" ", strip=True)
            if text:
                cells.append(text)

        if cells:
            row_text = " | ".join(cells)
            rows.append(row_text)

    return "\n".join(rows)


def _dedup_keep_order(items: List[str]) -> List[str]:
    seen = set()
    unique: List[str] = []
    for item in items:
        if item in seen:
            continue
        seen.add(item)
        unique.append(item)
    return unique


def split_pages_and_add_metadata(
    full_document: Document,
    file_name: str,
    *,
    basic_term_start: int,
    basic_term_end: int,
    special_term_start: int,
    special_term_end: int,
    output_format: OutputFormat = "html",
) -> List[Document]:
    """
    Upstage Document Parse ê²°ê³¼(ë‹¨ì¼ HTML Document)ë¥¼
    footerì˜ í˜ì´ì§€ ë§ˆì»¤ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³ 
    ê° í˜ì´ì§€ì— ë¬¸ì„œ/ì•½ê´€ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•œë‹¤.

    Args:
        full_document: Upstage Document Parserê°€ ë°˜í™˜í•œ ë‹¨ì¼ HTML Document.
        file_name: ì›ë³¸ PDF íŒŒì¼ëª…(í™•ì¥ì í¬í•¨).
        basic_term_start: ë³´í†µì•½ê´€ ì‹œì‘ í˜ì´ì§€ ë²ˆí˜¸(ë¬¸ì„œ footer ê¸°ì¤€).
        basic_term_end: ë³´í†µì•½ê´€ ì¢…ë£Œ í˜ì´ì§€ ë²ˆí˜¸(ë¬¸ì„œ footer ê¸°ì¤€).
        special_term_start: íŠ¹ë³„ì•½ê´€ ì‹œì‘ í˜ì´ì§€ ë²ˆí˜¸(ë¬¸ì„œ footer ê¸°ì¤€).
        special_term_end: íŠ¹ë³„ì•½ê´€ ì¢…ë£Œ í˜ì´ì§€ ë²ˆí˜¸(ë¬¸ì„œ footer ê¸°ì¤€).
        output_format: í˜ì´ì§€ë³„ ë¡œì»¬ ì €ì¥ íŒŒì¼ í¬ë§·(`html`, `text`, `markdown`).

    Returns:
        í˜ì´ì§€ë³„ë¡œ ë¶„ë¦¬ë˜ê³  metadataê°€ ì¶”ê°€ëœ `Document` ë¦¬ìŠ¤íŠ¸.

    Raises:
        ValueError: í•„ìˆ˜ ì¸ìê°€ ë¹„ì–´ ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°.
    """

    if (
        not full_document
        or not file_name
        or not basic_term_start
        or not basic_term_end
        or not special_term_start
        or not special_term_end
    ):
        raise ValueError("â—ï¸split_pages_and_add_metadata invalid params")

    result: List[Document] = []

    html = full_document.page_content
    soup = BeautifulSoup(html, "html.parser")

    root = soup.body if soup.body else soup

    page_docs: List[PageDoc] = []
    buf_text: List[str] = []
    buf_ids: List[str] = []

    for node in root.descendants:
        if not isinstance(node, Tag):
            continue

        # -------------------------
        # 1. í˜ì´ì§€ ë§ˆì»¤ ì²˜ë¦¬
        # -------------------------
        if node.name == "footer":
            raw = node.get_text(strip=True)
            m = _PAGE_RE.match(raw)
            if m:
                page_num = int(m.group(1))
                text = _norm_text("\n".join(buf_text))

                page_docs.append(
                    PageDoc(
                        page=page_num,
                        text=text,
                        anchor_ids=_dedup_keep_order(buf_ids),
                    )
                )

                buf_text.clear()
                buf_ids.clear()

            continue

        # -------------------------
        # 2. Table ì²˜ë¦¬ (ì¤‘ë³µ ë°©ì§€)
        # -------------------------
        if node.name == "table":
            table_text = _extract_table_text(node)
            if table_text:
                buf_text.append(table_text)

            node_id = node.get("id")
            if node_id:
                buf_ids.append(str(node_id))

            # table ë‚´ë¶€ëŠ” ì—¬ê¸°ì„œ ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ
            # descendants ìˆœíšŒ ì¤‘ë³µ ë°©ì§€ ìœ„í•´ skip
            continue

        # -------------------------
        # 3. ì¼ë°˜ í…ìŠ¤íŠ¸ íƒœê·¸ ì²˜ë¦¬
        # -------------------------
        if node.name in {
            "p",
            "h1",
            "h2",
            "h3",
            "li",
            "div",
            "span",
        }:
            text = node.get_text(" ", strip=True)
            if text:
                buf_text.append(text)

            node_id = node.get("id")
            if node_id:
                buf_ids.append(str(node_id))

    # ë§ˆì§€ë§‰ í˜ì´ì§€ footerê°€ ëˆ„ë½ëœ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ fallback
    if buf_text:
        inferred_page = page_docs[-1].page + 1 if page_docs else 1
        page_docs.append(
            PageDoc(
                page=inferred_page,
                text=_norm_text("\n".join(buf_text)),
                anchor_ids=_dedup_keep_order(buf_ids),
            )
        )

    total_pages = len(page_docs)
    insurer_code = file_name.split("_")[0]
    product_code = file_name.split("_")[1]
    product_name = product_name_by_code[
        (file_name.split("_")[0], file_name.split("_")[1])
    ]

    for page_doc in page_docs:
        if basic_term_start <= page_doc.page <= basic_term_end:
            term_type = "basic"
        elif special_term_start <= page_doc.page <= special_term_end:
            term_type = "special"
        else:
            term_type = ""

        new_doc = Document(
            page_content=page_doc.text,
            metadata={
                "source": {**full_document.metadata},
                "doc": {
                    "doc_type": "terms",  # ì•½ê´€
                    "file_name": file_name,  # ì•½ê´€ íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)
                    "insurer_code": insurer_code,  # ë³´í—˜ì‚¬ ì½”ë“œ (e.g. samsung, kb, meritz, ...)
                    "product_code": product_code,  # ë³´í—˜ ìƒí’ˆ ì½”ë“œ (e.g. 1, 2, 3, ...)
                    "product_name": product_name,  # ë³´í—˜ ìƒí’ˆ ëª… (e.g. á„†á…¦á„…á…µá„á…³ á„†á…¡á„‹á…³á†·á„ƒá…³á†«á„ƒá…³á†« á„‡á…¡á†«á„…á…§á„ƒá…©á†¼á„†á…®á†¯á„‡á…©á„’á…¥á†·)
                    "total_pages": total_pages,  # ì´ í˜ì´ì§€ ìˆ˜
                    "page": page_doc.page,  # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸
                    "anchor_ids": page_doc.anchor_ids,  # íŒŒì‹± ê³¼ì •ì—ì„œ ì‹ë³„ë˜ëŠ” HTML íƒœê·¸ ID (e.g. id="23")
                },
                "term_type": term_type,  # ì•½ê´€ ìœ í˜• (e.g. basic: ë³´í†µì•½ê´€, special: íŠ¹ë³„ì•½ê´€)'
            },
        )
        result.append(new_doc)

        DP_RESULTS_DIR = TERMS_DIR / file_name.split(".")[0]
        create_page_content_file(new_doc, DP_RESULTS_DIR, output_format)

    return result


def create_page_content_file(
    doc: Document,
    target_dir: Path,
    output_format: OutputFormat = "html",
):
    # rprint("create_page_content_file target_dir:", target_dir)
    target_dir.mkdir(parents=True, exist_ok=True)

    file_name_without_extension = doc.metadata["doc"]["file_name"].split(".")[0]
    OUTPUT_EXTENSION = output_extension_by_format.get(output_format)
    OUTPUT_FILE_NAME = f"{file_name_without_extension}_{doc.metadata['doc']['page']}.{OUTPUT_EXTENSION}"
    OUTPUT_FILE_PATH = target_dir / OUTPUT_FILE_NAME
    # rprint("ğŸ”—create_local_file OUTPUT_FILE_PATH:", OUTPUT_FILE_PATH)
    if OUTPUT_FILE_PATH.exists():
        # rprint("âš ï¸ create_local_file skipped (already exists)")
        return

    # rprint("ğŸš€create_local_file start")
    OUTPUT_FILE_PATH.write_text(doc.page_content, encoding="utf-8")
    # rprint("âœ…create_local_file done")
