from __future__ import annotations

import os
import re
from contextlib import nullcontext
from typing import Any, Dict, List, Tuple, Literal

from dotenv import load_dotenv
from pydantic import BaseModel, Field
from langchain.chat_models import init_chat_model

from langchain_core.documents import Document

try:
    from langsmith.run_helpers import tracing_context
except Exception:  # pragma: no cover - optional dependency fallback
    tracing_context = None

from app.agents.document_parser.constants import TERMS_DIR
from app.agents.document_parser.nodes.tagger.chunk_file import create_chunk_file
from app.agents.document_parser.nodes.tagger.tag_summary import (
    summarize_counts,
)


# `.env` íŒŒì¼ì˜ í™˜ê²½ë³€ìˆ˜ë¥¼ í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì— ë¡œë“œí•©ë‹ˆë‹¤.
# - ë¡œì»¬ ê°œë°œ ì‹œ `UPSTAGE_API_KEY`ë¥¼ ì½”ë“œì— í•˜ë“œì½”ë”©í•˜ì§€ ì•Šê³  ì‚¬ìš©í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
load_dotenv()

# ê°™ì€ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë°˜ë³µë˜ëŠ” ì´ˆê¸°í™”/íƒœê¹… ìš”ì²­ì„ ì¤„ì´ê¸° ìœ„í•œ ìºì‹œ
_STRUCTURED_LLM_CACHE: Dict[Tuple[str], Any] = {}
_TAG_RESULT_CACHE: Dict[Tuple[str, str, float], Dict[str, Any]] = {}
_TAG_RESULT_CACHE_MAX = int(os.getenv("TAGGING_RESULT_CACHE_MAX", "5000"))


def _get_tagging_langsmith_project_name() -> str | None:
    # tagging ì „ìš© í”„ë¡œì íŠ¸ëª…ì„ ë³„ë„ë¡œ ì§€ì •í•  ë•Œ ì‚¬ìš©:
    # 1) LANGSMITH_TAGGING_PROJECT
    # 2) LANGSMITH_PROJECT_TAGGING (fallback)
    project_name = os.getenv("LANGSMITH_TAGGING_PROJECT")
    if project_name:
        return project_name
    return None


# =========================
# 0) PoC ë¼ë²¨ ì •ì˜ (Label Set)
# =========================
# ë³¸ íŒŒì¼ì€ ê° chunkë¥¼ ì•„ë˜ 2ê°œ ì¶•ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
# 1) clause_type: ì•½ê´€ ì¡°í•­ì˜ ì„±ê²© (ë³´ì¥/ë©´ì±…/í•œë„ ë“±)
# 2) risk_domains: ì–´ë–¤ ì‹ ì²´/ì§ˆí™˜ ì˜ì—­ì¸ì§€
# ë§ˆì§€ë§‰ "other"ëŠ” ê·œì¹™/LLMìœ¼ë¡œ ë¶„ë¥˜ê°€ ì• ë§¤í•  ë•Œì˜ ì•ˆì „í•œ ê¸°ë³¸ê°’ì…ë‹ˆë‹¤.
CLAUSE_TYPES = [
    "coverage",  # ë³´ì¥/ë‹´ë³´: ë³´í—˜ì‚¬ê°€ ì‚¬ê³  ì‹œ ì±…ì„ì§€ê³  ë³´ìƒí•´ ì£¼ëŠ” êµ¬ì²´ì ì¸ ë²”ìœ„
    "exclusion",  # ë©´ì±… ì‚¬í•­: ë³´ìƒí•˜ì§€ ì•ŠëŠ” ì†í•´ (ì˜ˆ: ê³ ì˜ ì‚¬ê³ , ì „ìŸ ë“±)
    "other",  # ê¸°íƒ€
]
RISK_DOMAINS = [
    "head",
    "dental",
    "skin",
    "joint",
    "urinary",
    "eye",
    "digestive",
    "other",
]
TERM_TYPES = ["basic", "special", "other"]


# =========================
# 1) ê·œì¹™ ê¸°ë°˜(Regex) 1ì°¨ íƒœê¹…
# =========================
# í…ìŠ¤íŠ¸ì— íŠ¹ì • í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë¹ ë¥´ê²Œ ì´ˆê¸° ë¼ë²¨ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.
# ìˆœì„œê°€ ì¤‘ìš”í•©ë‹ˆë‹¤. ë¨¼ì € ë§¤ì¹­ëœ ê·œì¹™ì´ ìš°ì„  ì ìš©ë©ë‹ˆë‹¤.
# (ì˜ˆ: ë©´ì±… í‚¤ì›Œë“œê°€ ë‚˜ì˜¤ë©´ exclusionì„ ë¨¼ì € í™•ì •)
CLAUSE_TYPE_RULES: List[Tuple[str, str]] = [
    (r"(ë©´ì±…|ë³´ìƒí•˜ì§€\s*ì•Š|ì§€ê¸‰í•˜ì§€\s*ì•Š|ì œì™¸|ë¶€ì§€ê¸‰)", "exclusion"),
    (r"(ë³´ì¥|ì§€ê¸‰\s*ì‚¬ìœ |ë³´í—˜ê¸ˆ\s*ì§€ê¸‰|ë³´ìƒ)", "coverage"),
]

RISK_DOMAIN_RULES: List[Tuple[str, str]] = [
    (r"(ë‡Œ|ë‘ë¶€|ë¨¸ë¦¬|ê²½ë ¨|ì‹ ê²½)", "head"),
    (r"(ì¹˜ì•„|ì¹˜ì£¼|ìŠ¤ì¼€ì¼ë§|êµ¬ê°•)", "dental"),
    (r"(í”¼ë¶€|ìŠµì§„|ì•Œë ˆë¥´ê¸°|ê°€ë ¤ì›€)", "skin"),
    (r"(ê´€ì ˆ|ìŠ¬ê°œê³¨|íƒˆêµ¬|ê³ ê´€ì ˆ|ì‹­ìì¸ëŒ€)", "joint"),
    (r"(ë¹„ë‡¨|ë°©ê´‘|ìš”ë¡œ|ì‹ ì¥|ê²°ì„)", "urinary"),
    (r"(ëˆˆ|ê°ë§‰|ë°±ë‚´ì¥|ë§ë§‰)", "eye"),
    (r"(ìœ„|ì¥|ì†Œí™”|êµ¬í† |ì„¤ì‚¬)", "digestive"),
]


class ChunkTagOutput(BaseModel):
    clause_type: str = Field(...)
    risk_domains: List[str] = Field(min_length=1)
    confidence: float = Field(ge=0, le=1)
    notes: str | None = Field(default=None, max_length=200)


def rule_tag(text: str) -> Dict[str, Any]:
    """
    ê·œì¹™(ì •ê·œì‹)ë§Œ ì‚¬ìš©í•´ 1ì°¨ íƒœê¹…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Returns:
        {
            "clause_type": str,      # ì¡°í•­ ë¶„ë¥˜
            "risk_domains": list[str], # ìœ„í—˜ì˜ì—­(ë³µìˆ˜ ê°€ëŠ¥)
            "confidence": float,     # ê·œì¹™ ê¸°ë°˜ ì‹ ë¢°ë„(ë³´ìˆ˜ì )
            "method": "rule",        # ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ íƒœê¹…í–ˆëŠ”ì§€
            "notes": None            # í›„ì²˜ë¦¬/ì˜¤ë¥˜ë©”ëª¨ ì˜ì—­
        }
    """
    clause_type = "other"
    for pat, ctype in CLAUSE_TYPE_RULES:
        # ì²« ë§¤ì¹­ë§Œ ì±„íƒí•´ ê³¼ë„í•œ ë®ì–´ì“°ê¸°ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
        if re.search(pat, text):
            clause_type = ctype
            break

    domains: List[str] = []
    for pat, d in RISK_DOMAIN_RULES:
        if re.search(pat, text):
            domains.append(d)
    domains = sorted(set(domains)) or ["other"]

    # ê·œì¹™ ê¸°ë°˜ì€ ë¹ ë¥´ì§€ë§Œ ë¬¸ë§¥ ì´í•´ê°€ ì œí•œì ì´ë¯€ë¡œ
    # ì‹ ë¢°ë„(confidence)ëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
    confidence = 0.55 if clause_type != "other" else 0.25

    return {
        "clause_type": clause_type,
        "risk_domains": domains,
        "confidence": confidence,
        "method": "rule",
        "notes": None,
    }


# =========================
# 2) Solar Pro 2ë¡œ êµ¬ì¡°í™” ì¶œë ¥ íƒœê¹… (LLM Tagging)
# =========================
def llm_tag_solar_pro2(
    text: str,
    *,
    api_key: str,
    base_url: str = "https://api.upstage.ai/v1/solar",
    model: str = "solar-pro2",
    timeout_s: int = 30,
) -> Dict[str, Any]:
    """
    LLM(Upstage Solar Pro 2)ìœ¼ë¡œ êµ¬ì¡°í™” íƒœê¹…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    í•µì‹¬ í¬ì¸íŠ¸:
    - OpenAI í˜¸í™˜ Chat Completions APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - response_format(json_schema)ë¡œ ì¶œë ¥ í˜•ì‹ì„ ê°•ì œí•©ë‹ˆë‹¤.
      -> íŒŒì‹± ì•ˆì •ì„±ì´ ë†’ì•„ì§€ê³  í›„ì²˜ë¦¬ ì½”ë“œê°€ ë‹¨ìˆœí•´ì§‘ë‹ˆë‹¤.

    Args:
        text: íƒœê¹…í•  chunk ì›ë¬¸
        api_key: Upstage API Key
        base_url: Upstage OpenAI-compatible endpoint
        model: ì‚¬ìš©í•  ëª¨ë¸ëª…
        timeout_s: API íƒ€ì„ì•„ì›ƒ(ì´ˆ)

    Returns:
        rule_tagì™€ ë™ì¼ í˜•íƒœ + method="llm"
    """
    # init_chat_modelì€ LangChain í‘œì¤€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ
    # í”„ë¡œì íŠ¸ ì „ì²´ì—ì„œ ëª¨ë¸ í˜¸ì¶œ ë°©ì‹ê³¼ tracing êµ¬ì„±ì´ ì¼ê´€ë©ë‹ˆë‹¤.
    # ê¸°ì¡´ ì‹œê·¸ë‹ˆì²˜ í˜¸í™˜ì„ ìœ„í•´ api_key/base_url/timeout_s ì¸ìëŠ” ìœ ì§€í•˜ê³ ,
    # í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ ê´€ë¡€ëŒ€ë¡œ env ê¸°ë°˜ ì„¤ì •ì„ ìš°ì„  ì‚¬ìš©í•©ë‹ˆë‹¤.
    os.environ["UPSTAGE_API_KEY"] = api_key
    os.environ.setdefault("UPSTAGE_BASE_URL", base_url)

    system = (
        "ë„ˆëŠ” ë³´í—˜ ì•½ê´€ ë¬¸ì„œ ì²­í¬ë¥¼ ë¶„ë¥˜(tagging)í•˜ëŠ” ë¶„ë¥˜ê¸°ì•¼.\n"
        "ì•„ë˜ ë¼ë²¨ ì…‹ìœ¼ë¡œë§Œ ë¶„ë¥˜í•´.\n"
        f"- clause_type: {', '.join(CLAUSE_TYPES)}\n"
        f"- risk_domains: {', '.join(RISK_DOMAINS)}\n"
        "ë°˜ë“œì‹œ JSON schemaì— ë§ëŠ” JSONë§Œ ì¶œë ¥í•´."
    )

    user = (
        "ë‹¤ìŒ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë¼ë²¨ë§í•´ì¤˜.\n"
        "ë¶„ë¥˜ ê¸°ì¤€:\n"
        "- exclusion: ë³´ìƒí•˜ì§€ ì•ŠìŒ/ì§€ê¸‰í•˜ì§€ ì•ŠìŒ/ë©´ì±…/ì œì™¸/ë¶€ì§€ê¸‰\n"
        "- coverage: ë³´ì¥ ë²”ìœ„/ì§€ê¸‰ ì‚¬ìœ /ë³´í—˜ê¸ˆ ì§€ê¸‰/ë³´ìƒ\n"
        "- other: ìœ„ ë‘ ê°€ì§€ì— ëª…í™•íˆ í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ë‚´ìš©\n"
        "risk_domainsëŠ” í…ìŠ¤íŠ¸ì— ëª…ì‹œëœ ì‹ ì²´/ì§ˆí™˜ ì˜ì—­ì„ ì¶”ì •í•´.\n\n"
        f"TEXT:\n{text}"
    )

    # ëª¨ë¸/ìŠ¤í‚¤ë§ˆ ë˜í¼ëŠ” ìƒì„± ë¹„ìš©ì´ ìˆìœ¼ë¯€ë¡œ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    cache_key = (model,)
    structured_llm = _STRUCTURED_LLM_CACHE.get(cache_key)
    if structured_llm is None:
        llm = init_chat_model(model=model, temperature=0.0)
        structured_llm = llm.with_structured_output(ChunkTagOutput)
        _STRUCTURED_LLM_CACHE[cache_key] = structured_llm

    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": user},
    ]
    tagging_project = _get_tagging_langsmith_project_name()
    tracing_cm = (
        tracing_context(project_name=tagging_project)
        if tracing_context and tagging_project
        else nullcontext()
    )
    with tracing_cm:
        llm_response: ChunkTagOutput = structured_llm.invoke(messages)
    data = llm_response.model_dump()

    # ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ì—ì„œ íƒœê¹… ì¶œì²˜ë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ methodë¥¼ ëª…ì‹œí•©ë‹ˆë‹¤.
    data["method"] = "llm"
    return data


# =========================
# 3) ê²€ì¦/ë³´ì •(Validation/Override)
# =========================
def validate_and_override(text: str, tag: Dict[str, Any]) -> Dict[str, Any]:
    """
    ë³´í—˜ ë„ë©”ì¸ì—ì„œ ê°•ë ¥í•œ ì‹ í˜¸ëŠ” ë£°ë¡œ override.
    LLMì´ í—·ê°ˆë ¤ë„ ì—¬ê¸°ì„œ ì•ˆì •í™”.
    """
    # ê°•í•œ íŒ¨í„´ì€ ëª¨ë¸ ê²°ê³¼ë³´ë‹¤ ìš°ì„  ì ìš©í•´ ë¶„ë¥˜ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    # (ë³´í—˜ ì•½ê´€ì—ì„œ ë©´ì±…/ìê¸°ë¶€ë‹´/í•œë„ëŠ” ì˜ë¯¸ê°€ ë§¤ìš° í¬ê¸° ë•Œë¬¸)

    # ë©´ì±… ê°•ì œ ì‹ í˜¸
    if re.search(r"(ë³´ìƒí•˜ì§€\s*ì•Š|ì§€ê¸‰í•˜ì§€\s*ì•Š|ë©´ì±…|ì œì™¸|ë¶€ì§€ê¸‰)", text):
        tag["clause_type"] = "exclusion"
        tag["confidence"] = max(tag.get("confidence", 0.0), 0.85)
        tag["notes"] = (tag.get("notes") or "")[:150]

    # ë³´ì¥ ê°•ì œ ì‹ í˜¸
    if re.search(r"(ë³´ì¥|ì§€ê¸‰\s*ì‚¬ìœ |ë³´í—˜ê¸ˆ\s*ì§€ê¸‰|ë³´ìƒ)", text):
        if tag.get("clause_type") != "exclusion":
            tag["clause_type"] = "coverage"
            tag["confidence"] = max(tag.get("confidence", 0.0), 0.80)
            tag["notes"] = (tag.get("notes") or "")[:150]

    # risk_domains ìµœì†Œ 1ê°œ ë³´ì¥
    if not tag.get("risk_domains"):
        tag["risk_domains"] = ["other"]

    # enum ê²€ì¦(ì•ˆì „ì¥ì¹˜):
    # ì™¸ë¶€ ì…ë ¥/ëª¨ë¸ ì´ìƒ ì‘ë‹µ ë“±ìœ¼ë¡œ í—ˆìš©ë˜ì§€ ì•Šì€ ê°’ì´ ë“¤ì–´ì˜¤ë©´
    # íŒŒì´í”„ë¼ì¸ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ otherë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.
    if tag["clause_type"] not in CLAUSE_TYPES:
        tag["clause_type"] = "other"
    tag["risk_domains"] = [
        d if d in RISK_DOMAINS else "other" for d in tag["risk_domains"]
    ]
    if not tag["risk_domains"]:
        tag["risk_domains"] = ["other"]

    return tag


# =========================
# 4) ìµœì¢…: tag_chunk(text)
# =========================
def tag_chunk(
    text: str,
    *,
    upstage_api_key: str,
    use_llm_when: Literal[
        "always", "unknown_or_low_conf", "never"
    ] = "unknown_or_low_conf",
    llm_conf_threshold: float = 0.55,
) -> Dict[str, Any]:
    """
    chunk 1ê°œì— ëŒ€í•œ ìµœì¢… íƒœê¹… ì§„ì…ì ì…ë‹ˆë‹¤.

    ë™ì‘ ìˆœì„œ:
    1) ê·œì¹™ ê¸°ë°˜ 1ì°¨ íƒœê¹…(rule_tag)
    2) ì„¤ì •(use_llm_when, llm_conf_threshold)ì— ë”°ë¼ LLM íƒœê¹… ìˆ˜í–‰ ì—¬ë¶€ ê²°ì •
    3) validate_and_overrideë¡œ ë³´ì •/ê²€ì¦

    ì‹¤íŒ¨ ì²˜ë¦¬:
    - LLM í˜¸ì¶œì— ì‹¤íŒ¨í•˜ë©´ ê·œì¹™ ê¸°ë°˜ ê²°ê³¼ë¡œ fallbackí•©ë‹ˆë‹¤.
    """
    cache_key = (text, use_llm_when, llm_conf_threshold)
    cached = _TAG_RESULT_CACHE.get(cache_key)
    if cached is not None:
        # ìºì‹œëœ dictì˜ ì™¸ë¶€ ë³€í˜• ì˜í–¥ ë°©ì§€ë¥¼ ìœ„í•´ ë³µì‚¬ë³¸ ë°˜í™˜
        return dict(cached)

    base = rule_tag(text)

    should_llm = False
    if use_llm_when == "always":
        should_llm = True
    elif use_llm_when == "never":
        should_llm = False
    else:
        # unknown ë˜ëŠ” í™•ì‹ ë„ ë‚®ìœ¼ë©´ LLM í˜¸ì¶œ
        if base["clause_type"] == "other" or base["confidence"] < llm_conf_threshold:
            should_llm = True

    if should_llm:
        try:
            llm_out = llm_tag_solar_pro2(text, api_key=upstage_api_key)
            merged = llm_out
        except Exception as e:
            # ë„¤íŠ¸ì›Œí¬/API ì¼ì‹œ ì˜¤ë¥˜ê°€ ìˆì–´ë„ íŒŒì´í”„ë¼ì¸ì€ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.
            merged = base
            merged["notes"] = f"LLM failed: {type(e).__name__}"
    else:
        merged = base

    merged = validate_and_override(text, merged)

    # ë‹¨ìˆœ bounded cache (FIFO ìœ ì‚¬): ìµœëŒ€ í¬ê¸° ì´ˆê³¼ ì‹œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© 1ê°œ ì œê±°
    if len(_TAG_RESULT_CACHE) >= _TAG_RESULT_CACHE_MAX:
        _TAG_RESULT_CACHE.pop(next(iter(_TAG_RESULT_CACHE)))
    _TAG_RESULT_CACHE[cache_key] = dict(merged)

    return merged


def tag_chunks(
    chunks: List[Document],
    *,
    embedding_model: str = "solar-embedding-1-large",
    use_llm_when: Literal[
        "always", "unknown_or_low_conf", "never"
    ] = "unknown_or_low_conf",
    llm_conf_threshold: float = 0.55,
) -> List[Document]:
    """
    chunk(Document) ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ íƒœê¹… ë©”íƒ€ë°ì´í„°ë¥¼ í™•ì¥í•œ Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        chunks: splitterì—ì„œ ìƒì„±ëœ Document ë¦¬ìŠ¤íŠ¸
        insurer_code/product_code/pdf_name/doc_type: ë¬¸ì„œ ì‹ë³„ ë©”íƒ€ë°ì´í„°
        embedding_model: ì´í›„ ì„ë² ë”© ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ëª… ê¸°ë¡ìš©
        use_llm_when: "always" | "unknown_or_low_conf" | "never"
        llm_conf_threshold: LLM í˜¸ì¶œ ê¸°ì¤€ confidence ì„ê³„ê°’

    Returns:
        metadataê°€ í’ë¶€í•˜ê²Œ ì¶”ê°€ëœ List[Document]
    """
    # LLM í˜¸ì¶œ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ëª¨ë“œì—ì„œëŠ” API Keyê°€ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.
    upstage_api_key = os.getenv("UPSTAGE_API_KEY", "")
    if use_llm_when != "never" and not upstage_api_key:
        raise ValueError("UPSTAGE_API_KEY is not set. Please check your .env file.")

    tagged_chunks: List[Document] = []
    llm_used_count = 0
    for idx, chunk in enumerate(chunks):
        # tag_chunkëŠ” str ì…ë ¥ì„ ê¸°ëŒ€í•˜ë¯€ë¡œ page_contentë§Œ ì „ë‹¬í•©ë‹ˆë‹¤.
        chunk_text = chunk.page_content
        tag = tag_chunk(
            chunk_text,
            upstage_api_key=upstage_api_key,
            use_llm_when=use_llm_when,
            llm_conf_threshold=llm_conf_threshold,
        )

        # ìµœì¢… metadata ìŠ¤í‚¤ë§ˆ:
        # - clause: íƒœê¹… ê²°ê³¼
        # - indexing: ê²€ìƒ‰/ì¶”ì ìš© ìš´ì˜ ë©”íƒ€
        metadata = {
            **chunk.metadata,
            "clause": {
                "clause_type": tag["clause_type"],
                "risk_domains": tag["risk_domains"],
            },
            "indexing": {
                "chunk_id": f"chunk_{idx:06d}",
                "chunk_char_len": len(chunk_text),
                "embedding_model": embedding_model,
                "tag_method": tag["method"],
                "tag_confidence": tag["confidence"],
            },
        }

        # page_contentëŠ” ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ê³  metadataë§Œ í™•ì¥í•©ë‹ˆë‹¤.
        tagged_chunk = Document(page_content=chunk_text, metadata=metadata)
        tagged_chunks.append(tagged_chunk)
        target_dir = (
            TERMS_DIR / chunk.metadata["doc"]["file_name"].split(".")[0] / "chunks"
        )
        create_chunk_file(
            chunk=tagged_chunk,
            target_dir=target_dir,
        )

        if tag["method"] == "llm":
            llm_used_count += 1

        # ê¸´ ë°°ì¹˜ì—ì„œ ë©ˆì¶˜ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ ì•Šë„ë¡ ì§„í–‰ë¥ ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
        if (idx + 1) % 25 == 0 or idx == len(chunks) - 1:
            print(
                f"ğŸš€[tagging] processed {idx + 1}/{len(chunks)} chunks "
                f"(llm_used={llm_used_count})"
            )

    summarize_counts(
        tagged_chunks,
        clause_types=CLAUSE_TYPES,
        term_types=TERM_TYPES,
    )

    return tagged_chunks
